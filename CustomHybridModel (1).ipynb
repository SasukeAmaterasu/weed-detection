{"cells":[{"cell_type":"markdown","metadata":{"id":"o6UMbZdH2IFE"},"source":["Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19617,"status":"ok","timestamp":1730169554218,"user":{"displayName":"Jaswant Nayak","userId":"12218231795272059098"},"user_tz":-330},"id":"7zAWT-cGCGXS","outputId":"c7b18f83-5a72-41ca-bbb7-3d560cf2db88"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24447,"status":"ok","timestamp":1730169581307,"user":{"displayName":"Jaswant Nayak","userId":"12218231795272059098"},"user_tz":-330},"id":"KVcnC36c1iJD","outputId":"194a9c99-c9e4-4f9d-e132-e0eee37e18b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: numpy\u003e=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import os\n","import pandas as pd\n","!pip install opencv-python\n","import cv2 # import the cv2 module\n","import numpy as np # Import NumPy\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"WUFwMl0L2vW8"},"source":["Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZgh6wuf2xzZ"},"outputs":[],"source":["# Directories\n","image_data_dir = \"/content/drive/MyDrive/WeedCrop.v1i.yolov5pytorch/test/images\"\n","labels_data_dir = \"/content/drive/MyDrive/WeedCrop.v1i.yolov5pytorch/test/labels\"\n","\n","# Get image filenames\n","image_filenames = os.listdir(image_data_dir)\n","image_data = []\n","labels = []\n","\n","# Desired image dimensions\n","target_width = 128  # Example width\n","target_height = 128 # Example height\n","\n","# Iterate through image files and load corresponding label files\n","for filename in image_filenames:\n","    image_path = os.path.join(image_data_dir, filename)\n","    image = cv2.imread(image_path)\n","\n","    # Resize the image\n","    image = cv2.resize(image, (target_width, target_height))\n","\n","    image_data.append(image)\n","\n","    # Construct the path to the corresponding label file\n","    label_filename = os.path.splitext(filename)[0] + \".txt\"\n","    label_path = os.path.join(labels_data_dir, label_filename)\n","\n","    # Check if the label file exists\n","    if os.path.exists(label_path):\n","        # Read the label file\n","        with open(label_path, 'r') as file:\n","            bounding_boxes = file.readlines()\n","\n","        # Convert the bounding boxes to a structured format\n","        image_labels = []\n","        for box in bounding_boxes:\n","            box_data = list(map(float, box.strip().split()[1:]))  # Ignore the first column (class ID)\n","            image_labels.append(box_data)  # Add the bounding box coordinates\n","\n","        labels.append(image_labels)\n","    else:\n","        print(f\"Warning: Label file not found for {filename}. Skipping this image.\")\n","        image_data.pop()  # Remove the corresponding image data since no label file exists\n","\n","# Convert to NumPy arrays\n","image_data = np.array(image_data)\n","\n","\n","# One-hot encoding can be skipped since bounding boxes are used, not class labels\n","# If you had class labels, you would do:\n","# y = to_categorical(labels)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(image_data, labels, test_size=0.2, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"id":"mk5BX6224F_2"},"source":["Data Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jma_RdBB4LH6"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","datagen = ImageDataGenerator(\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    zoom_range=0.2,  # Add zooming\n","    shear_range=0.2  # Add shearing\n",")\n","datagen.fit(X_train)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bc_XpYtowoZF"},"outputs":[],"source":["import torch.nn as nn\n","\n","class CustomHybridModel(nn.Module): # Replace YourModelClassName with the actual name of your model class\n","    def __init__(self, *args, **kwargs):\n","        super(CustomHybridModel, self).__init__(*args, **kwargs)\n","        # ... your existing model initialization code ...\n","        self.dropout = nn.Dropout(p=0.5)\n","        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n","        self.fc2 = nn.Linear(512, 128)\n","        self.fc3 = nn.Linear(128, 8)\n","\n","\n","    def forward(self, x):\n","        # ... your existing forward pass code ...\n","        x = self.relu(self.fc1(x))\n","        x = self.dropout(x)  # Dropout applied here\n","        x = self.relu(self.fc2(x))\n","        x = self.fc3(x)  # Final output\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"FUM2hZUV4eeZ"},"source":["Design the custom hybrid model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rhx0hDI34l0s"},"outputs":[],"source":["class HybridWeedDetectionModel(nn.Module):\n","    def __init__(self):\n","        super(HybridWeedDetectionModel, self).__init__()\n","\n","        # CNN Component: Feature extraction\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.relu = nn.ReLU()\n","        self.batch_norm1 = nn.BatchNorm2d(32)\n","        self.batch_norm2 = nn.BatchNorm2d(64)\n","        self.batch_norm3 = nn.BatchNorm2d(128)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # ResNet Component: Residual blocks\n","        self.residual_block = nn.Sequential(\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128)\n","        )\n","\n","        # Custom Attention Mechanism\n","        self.attention_fc = nn.Sequential(\n","            nn.Linear(128 * 8 * 8, 512),  # Adaptive to input size\n","            nn.ReLU(),\n","            nn.Linear(512, 128 * 8 * 8),\n","            nn.Sigmoid()  # Output attention weights\n","        )\n","\n","        # Adaptive Pooling\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((16, 16))\n","\n","        # Fully Connected Layers\n","        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n","        self.fc2 = nn.Linear(512, 128)\n","        self.fc3 = nn.Linear(128, 8)  # 8 classes for weed detection\n","\n","    def forward(self, x):\n","        # CNN layers\n","        x = self.pool(self.batch_norm1(self.relu(self.conv1(x))))\n","        x = self.pool(self.batch_norm2(self.relu(self.conv2(x))))\n","        x = self.pool(self.batch_norm3(self.relu(self.conv3(x))))\n","\n","        # Residual Block\n","        identity = x  # Residual connection\n","        out = self.residual_block(x)\n","        x = self.relu(out + identity)  # Skip connection\n","\n","        # Attention Mechanism\n","        x_flat = x.view(x.size(0), -1)  # Flatten for attention\n","        attention_weights = self.attention_fc(x_flat).view(x.size())\n","        x = x * attention_weights  # Apply attention\n","\n","        # Adaptive Pooling\n","        x = self.adaptive_pool(x)\n","\n","        # Fully Connected Layers\n","        x = x.view(x.size(0), -1)  # Flatten before FC layers\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.fc3(x)  # Final output\n","\n","        return x\n","\n","# Instantiate the model\n","model = HybridWeedDetectionModel()\n"]},{"cell_type":"markdown","metadata":{"id":"3RV7jYEX4wFK"},"source":["Set Loss Function and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpM4esIp43Is"},"outputs":[],"source":["import torch.optim as optim\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"]},{"cell_type":"markdown","metadata":{"id":"IJeXzk4Y48V7"},"source":["Train the custom hybrid model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"XrGGrByM5A95"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Loss: 3.1988644003868103, Accuracy: 44.0%\n","Epoch 2/100, Loss: 0.5820453241467476, Accuracy: 75.0%\n","Epoch 3/100, Loss: 0.47872113436460495, Accuracy: 77.0%\n","Epoch 4/100, Loss: 0.6316577643156052, Accuracy: 60.0%\n","Epoch 5/100, Loss: 0.34985538572072983, Accuracy: 94.0%\n","Epoch 6/100, Loss: 0.22391851991415024, Accuracy: 94.0%\n","Epoch 7/100, Loss: 0.08277441747486591, Accuracy: 98.0%\n","Epoch 8/100, Loss: 0.008863021270371974, Accuracy: 100.0%\n","Epoch 9/100, Loss: 0.007583737489767373, Accuracy: 100.0%\n","Epoch 10/100, Loss: 0.0006414693998522125, Accuracy: 100.0%\n","Epoch 11/100, Loss: 0.0003035642043869302, Accuracy: 100.0%\n","Epoch 12/100, Loss: 1.801176631488488e-05, Accuracy: 100.0%\n","Epoch 13/100, Loss: 1.1123055173811736e-05, Accuracy: 100.0%\n","Epoch 14/100, Loss: 1.6216225816378937e-05, Accuracy: 100.0%\n","Epoch 15/100, Loss: 6.868027753625938e-06, Accuracy: 100.0%\n","Epoch 16/100, Loss: 4.019443622382823e-06, Accuracy: 100.0%\n","Epoch 17/100, Loss: 3.7938164211936964e-05, Accuracy: 100.0%\n","Epoch 18/100, Loss: 3.248346558848425e-06, Accuracy: 100.0%\n","Epoch 19/100, Loss: 1.1265460727827303e-05, Accuracy: 100.0%\n","Epoch 20/100, Loss: 1.5366574253050658e-06, Accuracy: 100.0%\n","Epoch 21/100, Loss: 4.34921638969854e-06, Accuracy: 100.0%\n","Epoch 22/100, Loss: 2.1280550583924196e-06, Accuracy: 100.0%\n","Epoch 23/100, Loss: 2.0516899894573726e-06, Accuracy: 100.0%\n","Epoch 24/100, Loss: 3.79403201122841e-06, Accuracy: 100.0%\n","Epoch 25/100, Loss: 4.4609298299747024e-06, Accuracy: 100.0%\n","Epoch 26/100, Loss: 1.0589100440938637e-06, Accuracy: 100.0%\n","Epoch 27/100, Loss: 1.2880092157274703e-06, Accuracy: 100.0%\n","Epoch 28/100, Loss: 2.6406533372380636e-05, Accuracy: 100.0%\n","Epoch 29/100, Loss: 1.490088890943042e-06, Accuracy: 100.0%\n","Epoch 30/100, Loss: 1.5674028723822175e-06, Accuracy: 100.0%\n","Epoch 31/100, Loss: 1.453782914495605e-06, Accuracy: 100.0%\n","Epoch 32/100, Loss: 1.2079138680398671e-06, Accuracy: 100.0%\n","Epoch 33/100, Loss: 2.4120073174316303e-06, Accuracy: 100.0%\n","Epoch 34/100, Loss: 1.7601698871771987e-06, Accuracy: 100.0%\n","Epoch 35/100, Loss: 1.7946484689446152e-06, Accuracy: 100.0%\n","Epoch 36/100, Loss: 6.0254777025647854e-06, Accuracy: 100.0%\n","Epoch 37/100, Loss: 1.1017379497602064e-06, Accuracy: 100.0%\n","Epoch 38/100, Loss: 3.4896006440021665e-06, Accuracy: 100.0%\n","Epoch 39/100, Loss: 6.555297133559179e-06, Accuracy: 100.0%\n","Epoch 40/100, Loss: 1.0067547790981735e-06, Accuracy: 100.0%\n","Epoch 41/100, Loss: 1.1892942879399016e-06, Accuracy: 100.0%\n","Epoch 42/100, Loss: 1.5639824994195806e-05, Accuracy: 100.0%\n","Epoch 43/100, Loss: 1.1129063448578336e-06, Accuracy: 100.0%\n","Epoch 44/100, Loss: 1.2498214232437022e-06, Accuracy: 100.0%\n","Epoch 45/100, Loss: 1.0775339518431792e-06, Accuracy: 100.0%\n","Epoch 46/100, Loss: 1.3653107799882491e-06, Accuracy: 100.0%\n","Epoch 47/100, Loss: 8.363255830090566e-07, Accuracy: 100.0%\n","Epoch 48/100, Loss: 2.755748568006311e-06, Accuracy: 100.0%\n","Epoch 49/100, Loss: 1.2321286675387455e-06, Accuracy: 100.0%\n","Epoch 50/100, Loss: 1.9305824139337346e-06, Accuracy: 100.0%\n","Epoch 51/100, Loss: 8.009345080495223e-07, Accuracy: 100.0%\n","Epoch 52/100, Loss: 6.863827053393834e-07, Accuracy: 100.0%\n","Epoch 53/100, Loss: 1.2749763982355944e-06, Accuracy: 100.0%\n","Epoch 54/100, Loss: 6.305035995524122e-07, Accuracy: 100.0%\n","Epoch 55/100, Loss: 1.1734619604908403e-06, Accuracy: 100.0%\n","Epoch 56/100, Loss: 8.633329713347848e-07, Accuracy: 100.0%\n","Epoch 57/100, Loss: 1.3848722772991096e-06, Accuracy: 100.0%\n","Epoch 58/100, Loss: 6.686874129968601e-07, Accuracy: 100.0%\n","Epoch 59/100, Loss: 7.245659499055535e-07, Accuracy: 100.0%\n","Epoch 60/100, Loss: 1.0281751769980474e-06, Accuracy: 100.0%\n","Epoch 61/100, Loss: 6.575108066897428e-07, Accuracy: 100.0%\n","Epoch 62/100, Loss: 7.962775576686454e-07, Accuracy: 100.0%\n","Epoch 63/100, Loss: 7.515744466779495e-07, Accuracy: 100.0%\n","Epoch 64/100, Loss: 7.906891994480247e-07, Accuracy: 100.0%\n","Epoch 65/100, Loss: 6.733439832373733e-07, Accuracy: 100.0%\n","Epoch 66/100, Loss: 8.754324904991506e-07, Accuracy: 100.0%\n","Epoch 67/100, Loss: 7.636819177037069e-07, Accuracy: 100.0%\n","Epoch 68/100, Loss: 4.647288704973107e-07, Accuracy: 100.0%\n","Epoch 69/100, Loss: 9.34113344897014e-07, Accuracy: 100.0%\n","Epoch 70/100, Loss: 9.397024669510756e-07, Accuracy: 100.0%\n","Epoch 71/100, Loss: 6.491297064314949e-07, Accuracy: 100.0%\n","Epoch 72/100, Loss: 9.229332746940599e-07, Accuracy: 100.0%\n","Epoch 73/100, Loss: 9.406252514665425e-07, Accuracy: 100.0%\n","Epoch 74/100, Loss: 5.252651504861205e-07, Accuracy: 100.0%\n","Epoch 75/100, Loss: 8.670557458678729e-07, Accuracy: 100.0%\n","Epoch 76/100, Loss: 5.103636624426144e-07, Accuracy: 100.0%\n","Epoch 77/100, Loss: 5.932500268812646e-07, Accuracy: 100.0%\n","Epoch 78/100, Loss: 4.926685654993435e-07, Accuracy: 100.0%\n","Epoch 79/100, Loss: 5.662427469133036e-07, Accuracy: 100.0%\n","Epoch 80/100, Loss: 7.35738087342952e-07, Accuracy: 100.0%\n","Epoch 81/100, Loss: 6.696192471622453e-07, Accuracy: 100.0%\n","Epoch 82/100, Loss: 6.733418729254481e-07, Accuracy: 100.0%\n","Epoch 83/100, Loss: 4.870808716361807e-07, Accuracy: 100.0%\n","Epoch 84/100, Loss: 1.186500277583491e-06, Accuracy: 100.0%\n","Epoch 85/100, Loss: 4.954622658814856e-07, Accuracy: 100.0%\n","Epoch 86/100, Loss: 8.596072689215362e-07, Accuracy: 100.0%\n","Epoch 87/100, Loss: 7.096663523498137e-07, Accuracy: 100.0%\n","Epoch 88/100, Loss: 1.0002382282436884e-06, Accuracy: 100.0%\n","Epoch 89/100, Loss: 9.005869117117982e-07, Accuracy: 100.0%\n","Epoch 90/100, Loss: 6.565808234881843e-07, Accuracy: 100.0%\n","Epoch 91/100, Loss: 6.6496265560545e-07, Accuracy: 100.0%\n","Epoch 92/100, Loss: 6.39817123726516e-07, Accuracy: 100.0%\n","Epoch 93/100, Loss: 5.802126921139461e-07, Accuracy: 100.0%\n","Epoch 94/100, Loss: 2.942926656146483e-06, Accuracy: 100.0%\n","Epoch 95/100, Loss: 8.167679723669607e-07, Accuracy: 100.0%\n","Epoch 96/100, Loss: 1.8076814036760425e-06, Accuracy: 100.0%\n","Epoch 97/100, Loss: 5.839376129301854e-07, Accuracy: 100.0%\n","Epoch 98/100, Loss: 6.724128098767324e-07, Accuracy: 100.0%\n","Epoch 99/100, Loss: 5.299214294041121e-07, Accuracy: 100.0%\n","Epoch 100/100, Loss: 7.338805474432775e-07, Accuracy: 100.0%\n","Test Accuracy: 60.0%\n"]}],"source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Assuming you have your data in NumPy arrays X_train, X_test, y_train, y_test\n","# If not, you need to load your data first\n","# Example using placeholder data:\n","import numpy as np\n","\n","# Replace these with your actual data loading and preprocessing\n","X_train = np.random.rand(100, 64, 64, 3)  # Example: 100 images of size 64x64 with 3 channels\n","X_test = np.random.rand(20, 64, 64, 3)    # Example: 20 images for testing\n","y_train = np.random.randint(0, 2, 100)    # Example: 100 labels (0 or 1)\n","y_test = np.random.randint(0, 2, 20)     # Example: 20 labels for testing\n","\n","# Check if GPU is available and use it\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Convert numpy arrays to PyTorch tensors\n","X_train_tensor = torch.Tensor(X_train).permute(0, 3, 1, 2)  # Rearrange to (batch_size, channels, height, width)\n","X_test_tensor = torch.Tensor(X_test).permute(0, 3, 1, 2)\n","y_train_tensor = torch.Tensor(y_train).long()\n","y_test_tensor = torch.Tensor(y_test).long()\n","\n","\n","\n","# Create DataLoader for batch processing\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Training loop\n","epochs = 100\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Calculate accuracy - Changed line\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item() # Comparing predicted labels directly with target labels\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {100 * correct / total}%\")\n","\n","# Validation loop\n","model.eval()  # Set the model to evaluation mode\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        # Comparing predicted labels directly with target labels\n","        correct += (predicted == labels).sum().item()\n","\n","print(f\"Test Accuracy: {100 * correct / total}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"828nbG4iDKlN"},"source":["Evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":722,"status":"ok","timestamp":1730113234998,"user":{"displayName":"Jaswant Nayak","userId":"12218231795272059098"},"user_tz":-330},"id":"na4n-q0GDPFL","outputId":"4b308e8e-e87c-4ede-9217-484cc86cc287"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test loss: 1.555803656578064\n","Test accuracy: 45.0\n"]}],"source":["import torch\n","\n","def evaluate(model, test_loader, device):\n","  \"\"\"\n","  Evaluates the model on the test data.\n","\n","  Args:\n","      model: The PyTorch model to evaluate.\n","      test_loader: The DataLoader for the test data.\n","      device: The device to run the evaluation on (e.g., 'cuda' or 'cpu').\n","\n","  Returns:\n","      A tuple containing the test loss and test accuracy.\n","  \"\"\"\n","  model.eval()  # Set the model to evaluation mode\n","  correct = 0\n","  total = 0\n","  loss = 0\n","  with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        # Comparing predicted labels directly with target labels\n","        correct += (predicted == labels).sum().item()\n","        loss += criterion(outputs, labels).item() # Calculate the loss for this batch\n","\n","  # Calculate average loss and accuracy\n","  avg_loss = loss / len(test_loader)\n","  accuracy = 100 * correct / total\n","\n","  return avg_loss, accuracy\n","\n","# Assuming you have your model, test_loader, and device defined\n","\n","# Evaluate the model\n","test_loss, test_accuracy = evaluate(model, test_loader, device)\n","\n","print(\"Test loss:\", test_loss)\n","print(\"Test accuracy:\", test_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"pnpSkScpDXeU"},"source":["Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1lTHno8nt7dEMlvSdFoyPE_mU-mncfXRD"},"id":"AQgccCFfDacz","outputId":"b38e4568-094f-4a1c-9198-f4f084d41a4c"},"outputs":[{"data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","import random\n","import cv2\n","import numpy as np\n","import torch\n","import os\n","\n","# Assuming the model, device, and X_test, y_test are already defined elsewhere in your code\n","\n","# Function to preprocess a single image\n","def preprocess_image(image, size=(64, 64)):\n","    # Resize the image\n","    resized_image = cv2.resize(image, size)\n","    # Convert image to float32 for better handling\n","    resized_image = resized_image.astype(np.float32) / 255.0\n","    # Normalize and convert the image to PyTorch tensor\n","    image_tensor = torch.Tensor(resized_image).permute(2, 0, 1).unsqueeze(0)  # (1, channels, height, width)\n","    return image_tensor\n","\n","# Load the test image\n","# Get a list of image files in the directory\n","image_path = \"/content/drive/MyDrive/WeedCrop.v1i.yolov5pytorch/test/images\"\n","image_files = [f for f in os.listdir(image_path) if os.path.isfile(os.path.join(image_path, f))]\n","\n","# Check if any image files were found\n","if not image_files:\n","    raise ValueError(\"No image files found in the directory.\")\n","\n","# Select a random image from the directory\n","random_image_file = random.choice(image_files)\n","test_image = cv2.imread(os.path.join(image_path, random_image_file))\n","\n","# Ensure the image was loaded correctly\n","if test_image is None:\n","    raise ValueError(\"Image loading failed. Check the image path and file.\")\n","\n","# Test label for reference (modify as needed)\n","test_label = 1  # Assuming 'Weed' for testing, change according to your dataset\n","\n","# Preprocess the image\n","test_image_tensor = preprocess_image(test_image).to(device)\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Make a prediction\n","with torch.no_grad():\n","    output = model(test_image_tensor)\n","    _, predicted_class = torch.max(output, 1)\n","\n","predicted_class = predicted_class.item()\n","\n","# Define a simple mapping for weed vs. non-weed classification\n","class_mapping = {0: \"Non-Weed\", 1: \"Weed\"}\n","\n","# --- Display the Original Image with Adjustments for Clarity ---\n","\n","# Ensure the image is in uint8 format for clear display\n","test_image_uint8 = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n","\n","# Create a figure with a higher DPI for better resolution\n","plt.figure(figsize=(8, 8), dpi=150)\n","\n","# Display the original image and the predicted label\n","plt.imshow(test_image_uint8)\n","plt.title(f\"Predicted: {class_mapping[predicted_class]}\", fontsize=16, pad=20)\n","plt.axis('off')  # Remove axes for a cleaner look\n","plt.tight_layout()  # Ensure layout is clean with no title-image overlap\n","plt.show()\n","\n","# Print actual class and prediction\n","print(f\"Actual class: {class_mapping[test_label]}\")\n","print(f\"Predicted class: {class_mapping[predicted_class]}\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNMHzS7uHAT+pMJqHbWDYEZ","mount_file_id":"1UDwovxFS_5fv7CFo90WziMDlYiT8pkDl","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}